# Assessment Type Field Implementation

## Overview

This document describes how to add an `assessment_type` field to all collections that have assessments (services, hardware, operating systems, mobile apps, self-hosted alternatives, and carriers) to track who performed the assessment.

## Field Specification

### Field Name
`assessment_type`

### Field Type
**Dropdown (Single Select)**

### Options
- `ai` - AI Generated
- `human` - Human Reviewed
- `brax_team` - Brax Team Verified

### Option Descriptions

#### AI Generated (`ai`)
- **When to use**: Assessment was automatically generated by AI tools (e.g., Perplexity, Claude, GPT)
- **Characteristics**:
  - Scores and recommendations generated by AI
  - Requires manual review and validation
  - Typically marked with `confidence: low` in Directus
  - Should remain in `draft` status until human-reviewed
- **Examples**:
  - Services added via Trigger.dev's `fetch-social-services` task
  - Automated research and scoring using AI APIs

#### Human Reviewed (`human`)
- **When to use**: Assessment reviewed and validated by a human contributor
- **Characteristics**:
  - AI-generated content has been verified
  - Scores adjusted based on manual research
  - Can be `published` status
  - Higher confidence level than AI-only
- **Examples**:
  - Community contributors reviewing AI-generated entries
  - Manual verification of technical specifications
  - Updating scores based on direct testing

#### Brax Team Verified (`brax_team`)
- **When to use**: Assessment officially reviewed and verified by the Brax/GoodPhone Foundation team
- **Characteristics**:
  - Highest level of verification
  - Scores align with GoodPhone Foundation's sovereignty framework
  - Detailed testing and validation performed
  - Reflects official project recommendations
- **Examples**:
  - Official recommendations on thecloudsucks.org
  - In-depth security and privacy audits
  - Services/hardware the foundation actively recommends

## Implementation Steps

### 1. Add Field to Each Collection in Directus

For each collection (`services`, `hardware_items`, `operating_systems`, `mobile_apps`, `selfhosted_alternatives`, `carriers`):

1. Go to **Settings** → **Data Model**
2. Select the collection
3. Click **Create Field**
4. Choose **Dropdown** field type
5. Configure:
   - **Key**: `assessment_type`
   - **Field**: Single Select
   - **Options**:
     ```
     ai - AI Generated
     human - Human Reviewed
     brax_team - Brax Team Verified
     ```
   - **Interface Options**:
     - Display: Dropdown
     - Icon: Add appropriate icon
     - Allow None: Yes (for backward compatibility)
   - **Default Value**: `ai` (for new AI-generated entries) or leave empty
   - **Required**: No (optional for backward compatibility)

### 2. Update TypeScript Types

Update type definitions in your codebase:

#### For Services (`types/services/index.ts`)
```typescript
export interface Service {
  // ... existing fields
  assessment_type?: 'ai' | 'human' | 'brax_team';
}
```

#### For Hardware (`types/hardware/index.ts`)
```typescript
export interface HardwareItem {
  // ... existing fields
  assessment_type?: 'ai' | 'human' | 'brax_team';
}
```

#### For Operating Systems (`types/operating-systems/index.ts`)
```typescript
export interface OperatingSystem {
  // ... existing fields
  assessment_type?: 'ai' | 'human' | 'brax_team';
}
```

#### For Mobile Apps (`types/apps/index.ts`)
```typescript
export interface MobileApp {
  // ... existing fields
  assessment_type?: 'ai' | 'human' | 'brax_team';
}
```

#### For Carriers
```typescript
export interface Carrier {
  // ... existing fields
  assessment_type?: 'ai' | 'human' | 'brax_team';
}
```

#### For Self-hosted Alternatives
```typescript
export interface SelfhostedAlternative {
  // ... existing fields
  assessment_type?: 'ai' | 'human' | 'brax_team';
}
```

### 3. Update Trigger.dev Tasks

Update all sync tasks to include the assessment_type field:

#### `trigger/lib/directus.ts` (for AI-generated services)
```typescript
const newService: any = {
  // ... existing fields
  assessment_type: 'ai', // AI-generated content
  confidence: 'low',
  notes_editorial: 'Auto-generated from Perplexity AI research. Requires manual review and validation.',
};
```

#### All sync tasks (`trigger/sync-*.ts`)
When fetching from Directus, include the field:
```typescript
fields: [
  // ... existing fields
  'assessment_type',
]
```

### 4. Update Typesense Schemas

Add the field to Typesense schemas for search/filtering:

#### `server/utils/typesense.ts`
```typescript
// For each collection schema
{
  name: 'services', // or carriers, hardware, etc.
  fields: [
    // ... existing fields
    { name: 'assessment_type', type: 'string', optional: true, facet: true },
  ]
}
```

### 5. Update UI Components

Add visual indicators in the frontend to show assessment type:

#### Badge Component
```vue
<template>
  <span 
    v-if="assessmentType" 
    :class="assessmentTypeClass"
    class="px-2 py-1 rounded text-xs font-medium"
  >
    {{ assessmentTypeLabel }}
  </span>
</template>

<script setup lang="ts">
const props = defineProps<{
  assessmentType?: 'ai' | 'human' | 'brax_team'
}>();

const assessmentTypeLabel = computed(() => {
  switch (props.assessmentType) {
    case 'ai': return 'AI Generated';
    case 'human': return 'Human Reviewed';
    case 'brax_team': return 'Brax Team Verified';
    default: return '';
  }
});

const assessmentTypeClass = computed(() => {
  switch (props.assessmentType) {
    case 'ai': return 'bg-yellow-100 text-yellow-800 dark:bg-yellow-900 dark:text-yellow-200';
    case 'human': return 'bg-blue-100 text-blue-800 dark:bg-blue-900 dark:text-blue-200';
    case 'brax_team': return 'bg-green-100 text-green-800 dark:bg-green-900 dark:text-green-200';
    default: return '';
  }
});
</script>
```

### 6. Update Detail Pages

Add assessment type indicator to detail pages:

```vue
<template>
  <div class="flex items-center gap-2">
    <span class="text-sm text-gray-600 dark:text-gray-400">
      Assessment:
    </span>
    <AssessmentTypeBadge :assessment-type="item.assessment_type" />
  </div>
</template>
```

### 7. Update Search/Filter

Add filtering by assessment type in search interfaces:

```typescript
// In search components
const filters = {
  // ... existing filters
  assessment_type: {
    label: 'Assessment Type',
    options: [
      { value: 'ai', label: 'AI Generated' },
      { value: 'human', label: 'Human Reviewed' },
      { value: 'brax_team', label: 'Brax Team Verified' }
    ]
  }
};
```

## Workflow Guidelines

### For AI-Generated Content
1. Set `assessment_type: 'ai'`
2. Set `status: 'draft'`
3. Set `confidence: 'low'`
4. Add editorial note explaining it needs review

### For Human Review
1. Review AI-generated content
2. Update `assessment_type: 'human'`
3. Update `status: 'published'` if ready
4. Adjust `confidence: 'medium'`
5. Update scores if needed

### For Brax Team Verification
1. Complete thorough verification
2. Update `assessment_type: 'brax_team'`
3. Set `status: 'published'`
4. Set `confidence: 'high'`
5. Add detailed notes on verification process

## Backward Compatibility

- Field is optional, so existing records won't break
- Existing records can be bulk-updated based on:
  - `confidence` field values
  - `notes_editorial` content
  - Manual review status
- Default to `null` for legacy records until reviewed

## Migration Script Example

```typescript
// Bulk update existing records
// AI-generated (confidence: low)
UPDATE services 
SET assessment_type = 'ai' 
WHERE confidence = 'low' AND notes_editorial LIKE '%AI%';

// Human reviewed (confidence: medium)
UPDATE services 
SET assessment_type = 'human' 
WHERE confidence = 'medium';

// Brax team verified (confidence: high or specific editorial notes)
UPDATE services 
SET assessment_type = 'brax_team' 
WHERE confidence = 'high' OR notes_editorial LIKE '%Brax Team%';
```

## Benefits

1. **Transparency**: Users can see the verification level
2. **Trust Building**: Brax Team verification provides official endorsement
3. **Quality Control**: Easy to identify content needing review
4. **Filtering**: Users can filter by verification level
5. **Workflow**: Clear progression from AI → Human → Brax Team
6. **Analytics**: Track percentage of verified content

## Future Enhancements

- Add `assessment_date` to track when verification occurred
- Add `assessed_by` to track which team member verified
- Create automated workflows to notify team when AI content needs review
- Dashboard showing content verification status
- Gamification for community contributors reaching "Human Reviewed" status
